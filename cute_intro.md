# Developing CUDA Kernels for Accelerated Matrix Multiplication on NVIDIA Hopper Architecture using the CUTLASS Library



# ä½¿ç”¨ cuda ç¼–å†™çŸ©é˜µä¹˜æ³•
## ç‰ˆæœ¬1 naiveç‰ˆæœ¬ 
```Python
for i in range(0, M):
  for j in range(0, N):
     for k in range(0, K):
         c[i][j] += A[i][k] * B[k][j]
```      
## ç‰ˆæœ¬2 shared memory + åˆå¹¶è®¿é—®å­˜å‚¨ + float4
![image](https://github.com/user-attachments/assets/0cf59666-50e1-48da-8644-88b123975d79)
### è®¿å­˜æ–¹å¼1ï¼Œæ— æ³•è¿ç»­è®¿å­˜
```
for (int ph = 0; ph < width; ph++)
{

    for (int index_q = 0; index_q < TM; index_q++)
    {
        for (int index_k = 0; index_k < BK; index_k++)
        {
            if (indA + index_q < M && index_k + ph * BK < K)
            {
                SA[(threadIdx.x * TM + index_q) * BK + index_k] = dA[(indA + index_q) * K + index_k + ph * BK];
            }
            else
            {
                SA[(threadIdx.x * TM + index_q) * BK + index_k] = 0.0f;
            }
        }
    }
```
### è®¿å­˜æ–¹å¼2ï¼Œå¯ä»¥è¿ç»­è®¿å­˜
```
int indA = TM * (blockIdx.x * blockDim.x);
int indB = TN * (blockIdx.y * blockDim.y);
int width = (K + BK - 1) / BK;
float tmp[TM * TN] = {0.0f};
int tid = threadIdx.x + threadIdx.y * blockDim.x;
int smem_a_m = tid % 128;
int smem_a_k = tid / 128;
int smem_b_k = tid % 8;
int smem_b_n = tid / 8;
for (int ph = 0; ph < width; ph++)
{

    if (indA + smem_a_m < M && smem_a_k + ph * BK < K)
    {
        SA[smem_a_m * BK + smem_a_k] = dA[(indA + smem_a_m) * K + smem_a_k + ph * BK];
    }
    else
    {
        SA[smem_a_m * BK + smem_a_k] = 0.0f;
    }
    if (indB + smem_b_n < N && smem_b_k + ph * BK < K)
    {

        SB[smem_b_k * BN + smem_b_n] = dB[(smem_b_k + ph * BK) * N + indB + smem_b_n];
    }
    else
    {
        SB[smem_b_k * BN + smem_b_n] = 0.0f;
    }

```
è®¿å­˜æ–¹å¼1å’Œè®¿å­˜æ–¹å¼2å¯¹æ¯” 
![image](https://github.com/user-attachments/assets/38f90267-9a5c-4074-83c2-b0567ca9132f)

## ç‰ˆæœ¬3 - shard memory bank conflict
``` C++
(float4 &)a[0] = (float4 &)dA[(indA + smem_a_m) * K + 4 * smem_a_k + ph * BK];
for (int id = 0; id < 4; id++)
{
    if (indA + smem_a_m >= M || ph * BK + 4 * smem_a_k + id >= K)
    {
        SA[(4 * smem_a_k + id) * BM + smem_a_m] = 0.0f;
    }
    else
    {
        SA[(4 * smem_a_k + id) * BM + smem_a_m] = a[id];
    }
}
```

## ç‰ˆæœ¬4 å‘é‡å†…ç§¯ vs å‘é‡å¤–ç§¯
### å‘é‡å†…ç§¯
```
for (int index_q = 0; index_q < TM; index_q++)
{
    for (int index_v = 0; index_v < TN; index_v++)
    {
        int reg_c_m = threadIdx.x * TM + index_q;
        int reg_c_n = threadIdx.y * TN + index_v;
        for (int index_k = 0; index_k < BK; index_k++)
        {
            tmp[index_q * TN + index_v] += SA[index_k * BM + reg_c_m] * SB[index_k * BN + reg_c_n];
        }
    }
}
```
### å‘é‡å¤–ç§¯
```
for (int index_k = 0; index_k < BK; index_k++)
{
    (float4 &)com_a[0] = (float4 &)SA[index_k * BM + threadIdx.x * TM];
    (float4 &)com_a[4] = (float4 &)SA[index_k * BM + threadIdx.x * TM + 4];
    (float4 &)com_b[0] = (float4 &)SB[index_k * BN + threadIdx.y * TN];
    (float4 &)com_b[4] = (float4 &)SB[index_k * BN + threadIdx.y * TN + 4];
    for (int index_q = 0; index_q < TM; index_q++)
    {
        for (int index_v = 0; index_v < TN; index_v++)
        {
            tmp[index_q * TN + index_v] += com_a[index_q] * com_b[index_v];
        }
    }
}
```
å°†SAå’ŒSBçš„æ•°æ®æŒ‰ç…§è¡Œï¼ŒåŠ è½½åˆ°å¯„å­˜å™¨ï¼Œåç»­è®¡ç®—ä»å¯„å­˜å™¨åŠ è½½
## ç‰ˆæœ¬5 åŠ è½½æ•°æ®å’ŒçŸ©é˜µè¿ç®—è¿›è¡Œæµæ°´
åŠ è½½ä¸‹æ¬¡è¦è®¡ç®—çš„æ•°æ®ï¼Œç„¶åè¿›è¡Œæœ¬æ¬¡çš„mm

###  åŸå› 
- æœ¬è´¨æ˜¯GPUå­˜å‚¨çš„å±‚æ¬¡ç»“æ„ï¼Œè®¿é—®ä¸åŒå±‚çº§çš„å­˜å‚¨é€Ÿåº¦ä¸åŒã€‚
- çŸ©é˜µè¿ç®—çš„ç‰¹ç‚¹ï¼ŒCçŸ©é˜µä¸­çš„æ¯ä¸ªå…ƒç´ éœ€è¦ï¼Œä¾èµ–è®¿é—®AçŸ©é˜µè¡Œä»¥åŠBçŸ©é˜µä¸­çš„åˆ—ã€‚


## cutlass 
![image](https://github.com/user-attachments/assets/7dc4e00a-25b4-48ef-9454-6f9d401b03d2)

![image](https://github.com/user-attachments/assets/1bd4a1a6-2454-44ba-bf61-6a08725c1e17)


With reference to the functionality exposed by CUTLASS, performance is sensitive to many parameters such as:

å†³å®šçŸ©é˜µä¹˜æ³•çš„å› ç´ åŒ…å«ä»¥ä¸‹

  Shape of matrices (tall, skinny, or square);
- Layout of matrices (row or column);
- Number of warps per thread block;
- Thread block shape;
- Thread cluster shape;
- Number of pipeline stages in the software pipelining optimization;
- Chosen precision (TF32 vs FP32 vs FP16 vs FP8);
- Usage of special MMA instructions like WGMMA or TMA.

## Cutalss API å±‚æ¬¡

A basic listing of CUTLASS-based Matmul is described in Listing 1. Apart from CuTe, CUTLASS has the
following 3 important APIs for GEMM, each corresponding to a distinct level of the GPU memory hierarchy [12]:


### (1) Device API;
The Device API is the highest-level API. It is invoked from the Host (i.e., CPU) and does not have any detail
about the specifics of the Matmul implementation. This API is used by host-side .cu code to invoke CUTLASSâ€™s
GEMM kernels, much like cuBLAS API.

è®¾å¤‡APIæ˜¯æœ€é«˜çº§åˆ«çš„APIã€‚å®ƒä»ä¸»æœºï¼ˆå³CPUï¼‰è°ƒç”¨ï¼Œä¸æ¶‰åŠMatmulå®ç°çš„å…·ä½“ç»†èŠ‚ã€‚æ­¤APIç”±ä¸»æœºç«¯çš„.cuä»£ç è°ƒç”¨ï¼Œç”¨äºè°ƒç”¨CUTLASSçš„GEMMå†…æ ¸ï¼Œç±»ä¼¼äºcuBLAS APIã€‚
åœ¨ä¸»æœºä¸Šå¯åŠ¨kernel

### (2) Kernel API;
The Kernel API embodies the entire grid. It thus schedules the collectives and is responsible for tiling the input
matrices into row and column panels, loading the references to them and invoking the GEMM and the epilogues.
Fusion of epilogues with GEMM happens at the Kernel API level.

å†…æ ¸APIä½“ç°äº†æ•´ä¸ªç½‘æ ¼ã€‚å› æ­¤ï¼Œå®ƒè´Ÿè´£è°ƒåº¦é›†ä½“æ“ä½œï¼Œå¹¶å°†è¾“å…¥çŸ©é˜µåˆ‡åˆ†ä¸ºè¡Œå’Œåˆ—é¢æ¿ï¼ŒåŠ è½½è¿™äº›é¢æ¿çš„å¼•ç”¨å¹¶è°ƒç”¨GEMMï¼ˆé€šç”¨çŸ©é˜µä¹˜æ³•ï¼‰å’Œå°¾å£°æ“ä½œã€‚GEMMä¸å°¾å£°æ“ä½œçš„èåˆåœ¨å†…æ ¸APIçº§åˆ«å‘ç”Ÿã€‚

### (3) Collective API.
The Collective API embodies a thread block or a cluster of thread blocks (from Hopper architecture onwards).
Collective APIs can be used to construct a GEMM as well as the epilogue to be fused with GEMM. The default
epilogue simply writes out the accumulator of GEMM from register memory to global memory. CUTLASS defines
several other typical operations such as linear scaling and clamping; other device-side function call operators
may also be used to perform custom operations.

Collective API ä½“ç°äº†ä¸€ä¸ªçº¿ç¨‹å—æˆ–ä¸€ä¸ªçº¿ç¨‹å—é›†ç¾¤ï¼ˆä»Hopperæ¶æ„å¼€å§‹ï¼‰ã€‚Collective API å¯ç”¨äºæ„å»º GEMM ä»¥åŠä¸ GEMM èåˆçš„å°¾éƒ¨æ“ä½œã€‚é»˜è®¤çš„å°¾éƒ¨æ“ä½œç®€å•åœ°å°† GEMM çš„ç´¯åŠ å™¨ä»å¯„å­˜å™¨å†…å­˜å†™å…¥å…¨å±€å†…å­˜ã€‚CUTLASS å®šä¹‰äº†å‡ ç§å…¶ä»–å…¸å‹æ“ä½œï¼Œå¦‚çº¿æ€§ç¼©æ”¾å’Œé™åˆ¶ï¼›è¿˜å¯ä»¥ä½¿ç”¨å…¶ä»–è®¾å¤‡ç«¯å‡½æ•°è°ƒç”¨æ“ä½œç¬¦æ¥æ‰§è¡Œè‡ªå®šä¹‰æ“ä½œã€‚


```c++
using ElementA = float; // Element type for A matrix operand
using ElementB = float; // Element type for B matrix operand
using ElementC = float; // Element type for C and D matrix operands
using ArchTag = cutlass::arch::Sm90; // Tag indicating the SM
using OperatorClass = cutlass::arch::OpClassTensorOp; // Operator class tag
using TileShape = Shape<_128,_128,_32>; // Threadblock-level tile size
using ClusterShape = Shape<_1,_2,_1>; // Shape of the threadblocks in a cluster
```
## Collective API
```
using CollectiveMainloop = typename cutlass::gemm::collective::CollectiveBuilder<
  ArchTag, OperatorClass,
  ElementA, RowMajor, 4,
  Developing CUDA Kernels for GEMM on Hopper using CUTLASS â€¢ 9
  ElementB, ColumnMajor, 4,
  ElementAccumulator,
  TileShape, ClusterShape,
  cutlass::gemm::collective::StageCountAuto,
  cutlass::gemm::collective::KernelScheduleAuto
>::CollectiveOp;
using CollectiveEpilogue = typename cutlass::epilogue::collective::
  CollectiveBuilder<
  cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
  TileShape, ClusterShape,
  cutlass::epilogue::collective::EpilogueTileAuto,
  ElementC, ElementC,
  ElementC, ColumnMajor, 4,
  ElementC, ColumnMajor, 4,
  cutlass::epilogue::collective::EpilogueScheduleAuto
>::CollectiveOp;
```
## Kernel API
```c++
using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
  Shape<int,int,int>, // Indicates ProblemShape
  CollectiveMainloop,
  CollectiveEpilogue
>;
```
## Device API
```c++
using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
```

## Tiled Matrix Multiplication Using CuTe
CuTe is another API within the CUTLASS API that provides even more flexibility to develop GEMM kernels. It specifically introduces the concept of Shapes and Layouts, using which programmers can define the different levels of tiling explicitly. Additionally, it provide APIs to:

ä½¿ç”¨cuteè¿›è¡ŒçŸ©é˜µä¹˜æ³•çš„workflowï¼Œæä¾›äº†ä»¥ä¸‹çš„API
- (a) Convert matrices in to tensors and partition them;
- (b) Access the tiles of a tensor that belong to a thread block (local_tiles);
- (c) Make a local partition of a tensor that belongs to a thread within a thread block (local_partition);
- (d) Copy between GEMM, SMEM and RMEM (copy);
- (e) Multiply tensors with special Matmul instructions like WGMMA (gemm);
- (f) Synchronize between thread clusters;
- (g) Make special swizzle layouts for shared memory.


###  plain SIMT mul-add
 
```
template <class MShape, class NShape, class KShape,
class TA, class AStride, class ABlockLayout, class AThreadLayout,
class TB, class BStride, class BBlockLayout, class BThreadLayout,
class TC, class CStride, class CBlockLayout, class CThreadLayout,
class Alpha, class Beta>
__global__ static
void
gemm_device(MShape M, NShape N, KShape K,
TA const* A, AStride dA, ABlockLayout blockA, AThreadLayout tA,
TB const* B, BStride dB, BBlockLayout blockB, BThreadLayout tB,
TC * C, CStride dC, CBlockLayout blockC, CThreadLayout tC,
Alpha alpha, Beta beta)
{
using namespace cute;
using X = Underscore;
// Shared memory buffers.
__shared__ TA smemA[cosize_v<ABlockLayout>];
__shared__ TB smemB[cosize_v<BBlockLayout>];
auto sA = make_tensor(make_smem_ptr(smemA), blockA);
auto sB = make_tensor(make_smem_ptr(smemB), blockB);
// Represent the full tensors.
auto mA = make_tensor(make_gmem_ptr(A), make_shape(M,K), dA);
auto mB = make_tensor(make_gmem_ptr(B), make_shape(N,K), dB);
auto mC = make_tensor(make_gmem_ptr(C), make_shape(M,N), dC);
// Get the appropriate blocks for this thread block.
auto MT = size<0>(sA);
auto NT = size<0>(sB);
auto KT = size<1>(sB);
auto gA = local_tile(mA, make_shape(MT, KT), make_coord(blockIdx.x, _));
auto gB = local_tile(mB, make_shape(NT, KT), make_coord(blockIdx.y, _));
auto gC = local_tile(mC, make_shape(MT, NT), make_coord(blockIdx.x, blockIdx.y);
// Define partitioned views of GMEM and SMEM for COPY

auto tAgA = local_partition(gA, tA, threadIdx.x);
auto tAsA = local_partition(sA, tA, threadIdx.x);
auto tBgB = local_partition(gB, tB, threadIdx.x);
auto tBsB = local_partition(sB, tB, threadIdx.x);
// Define partitioned views of SMEM for GEMM.
// Partition sA (M,K) by the rows of tC.
auto tCsA = local_partition(sA, tC, threadIdx.x, Step<_1, X>{});
// Partition sB (N,K) by the cols of tC.
auto tCsB = local_partition(sB, tC, threadIdx.x, Step< X,_1>{});
// Partition gC (M,N) by the tile of tC.
auto tCgC = local_partition(gC, tC, threadIdx.x, Step<_1,_1>{});
// Allocate the accumulators (RMEM).
auto tCrC = make_fragment_like(tCgC);
// Clear the accumulators
clear(tCrC);


// Data is copied from GMEM to SMEM using the COPY views.
// gemm(.) operates on the GEMM views.
auto k_max = size<2>(tAgA);
for (int k = 0; k < k_max; ++k) {
// Copy GMEM to SMEM.
copy(tAgA(_,_,k), tAsA);
copy(tBgB(_,_,k), tBsB);
cp_async_fence();
cp_async_wait<0>();
__syncthreads();
// Compute GEMM on SMEM.
// Accumulate to registers.
gemm(tCsA, tCsB, tCrC);
__syncthreads();
}
 
// Epilogue fusion goes here.
for (int i = 0; i < size(tCgC); ++i)
{
tCgC(i) = tCrC(i);
}
}
```

çŸ©é˜µä¹˜æ³•è®¡ç®—çš„å…³é”®éƒ¨åˆ†åœ¨å¾ªç¯ä¸­åˆ—å‡ºã€‚è¯¥ Kernel è®¡ç®—çŸ©é˜µ ğ´ å’Œ ğµ çš„ä¹˜ç§¯ï¼Œç»“æœä¸º ğ¶ã€‚çŸ©é˜µ ğ´ å’Œ ğµ å¦‚å›¾ 4 æ‰€ç¤ºè¿›è¡Œäº†åˆ‡å—å¤„ç†ã€‚å¯¹äºçŸ©é˜µ ğ¶ çš„åˆ‡å—å¤„ç†ï¼Œåœ¨ naive Matmul å®ç°ä¸­å·²ä½œè¿‡è®¨è®ºã€‚naive Matmul å’Œæ­¤ Matmul ä¹‹é—´çš„ä¸»è¦åŒºåˆ«æœ‰ï¼š

(1) çŸ©é˜µå…ƒç´ é¦–å…ˆé€šè¿‡å¼‚æ­¥å¤åˆ¶æ“ä½œä»å…¨å±€å†…å­˜ï¼ˆGMEMï¼‰å¸¦å…¥å…±äº«å†…å­˜ï¼ˆSMEMï¼‰ï¼›
(2) ç»“æœçŸ©é˜µ ğ¶ å­˜å‚¨åœ¨å¯„å­˜å™¨å†…å­˜ï¼ˆRMEMï¼‰ä¸­ï¼Œå¹¶æœ€ç»ˆåœ¨å°¾å£°é˜¶æ®µå†™å›å…¨å±€å†…å­˜ï¼ˆGMEMï¼‰ï¼›
(3) è®¡ç®—ä¹Ÿæ²¿ç€ ğ¾ ç»´åº¦è¿›è¡Œäº†åˆ‡å—å¤„ç†ã€‚è¿™ä½¿å¾—ç¬¬ (1) æ­¥æˆä¸ºå¯èƒ½ï¼Œå› ä¸ºä¸æ•´ä¸ªè¡Œæˆ–åˆ—é¢æ¿ç›¸æ¯”ï¼Œğ´ å’Œ ğµ çš„åˆ‡å—è¶³å¤Ÿå°ï¼Œå¯ä»¥é€‚åº”å…±äº«å†…å­˜ã€‚

åœ¨åˆ—è¡¨ 2 ä¸­éœ€è¦å¼ºè°ƒçš„ CuTe çš„å…³é”® API æ˜¯ï¼š

(1) local_tileï¼šå°†çº¿ç¨‹å—æœ¬åœ°çš„åˆ‡å—æå–åˆ°å¼ é‡ä¸­ã€‚
(2) local_partitionï¼šå°†çº¿ç¨‹å—ä¸­çº¿ç¨‹æœ¬åœ°çš„å…ƒç´ æå–åˆ°å¼ é‡ä¸­ã€‚
(3) make_fragment_like: ç”³è¯·å¯„å­˜å™¨ï¼Œ
(4) make_tensorï¼šåŸºäºæ˜¾å­˜åœ°å€ï¼Œæ„å»ºtensor
(5) make_coordï¼šæ„å»ºè®¿é—®åæ ‡
(6) copyï¼šå°†æ•°æ®ä»å…¨å±€æ˜¾å­˜æ‹·è´åˆ°å…±äº«æ˜¾å­˜ï¼Œä¸åŒæ¶æ„çš„ç¡¬ä»¶è®¾å¤‡ç”±ä¸åŒçš„å®ç°
(7) gemmï¼šçŸ©é˜µä¹˜æ³•ï¼Œä¸åŒæ¶æ„çš„ç¡¬ä»¶è®¾å¤‡ç”±ä¸åŒçš„å®ç°ï¼Œä¾‹å¦‚SIMT ä»¥åŠ Tensor Core

ä¸€æ—¦ä½¿ç”¨ CuTe API è·å–äº†æœ¬åœ°åˆ‡å—ï¼Œå°±å¯ä»¥ä½¿ç”¨ GEMM API å°†å¯¹åº”çš„ ğ´ å’Œ ğµ åˆ‡å—ç›¸ä¹˜ï¼Œå¹¶å°†ç»“æœç´¯åŠ åˆ° ğ¶ çŸ©é˜µï¼ˆåœ¨å¯„å­˜å™¨ä¸­ï¼‰ã€‚åœ¨æ²¿ç€ ğ¾ ç»´åº¦å¤„ç†å®Œæœ€åä¸€ä¸ªåˆ‡å—åï¼Œç»“æœ ğ¶ éšåè¢«å†™å…¥ GMEMã€‚ 
CuTe ä¸­çš„ä¸€ä¸ªé‡è¦ç‰¹æ€§æ˜¯å¼ é‡çš„è§†å›¾ï¼ˆåœ¨ C++ æ¦‚å¿µçš„æ„ä¹‰ä¸Šï¼‰ã€‚åœ¨å¤åˆ¶æ“ä½œæœŸé—´ï¼Œä»å…¨å±€å†…å­˜è¯»å–æ•°æ®åˆ°å…±äº«å†…å­˜æ—¶ï¼Œè¾“å…¥å¼ é‡ä½¿ç”¨åŸºäº AThreadLayoutA (tA) å’Œ BThreadLayout (tB) çš„è§†å›¾ã€‚ä¾‹å¦‚ï¼Œè¿™æ ·çš„è§†å›¾æ˜¯ä¸ºäº†æ”¹å–„å…¨å±€å†…å­˜åŠ è½½çš„åˆå¹¶æ•ˆæœã€‚ç„¶è€Œï¼Œåœ¨ GEMM æ“ä½œæœŸé—´ï¼Œä½¿ç”¨çš„æ˜¯åŸºäº CThreadLayout (tC) çš„è§†å›¾ã€‚è¿™ç§çº¿ç¨‹åˆ°æ•°æ®çš„æ˜ å°„å¯ä»¥æé«˜çŸ©é˜µä¹˜æ³•è®¡ç®—çš„æ€§èƒ½ã€‚ä½†å¯èƒ½ä¸ä¼šå¯¼è‡´å¯¹å…¨å±€å†…å­˜çš„åˆå¹¶å­˜å‚¨ã€‚åŸå§‹å…±äº«å†…å­˜å¯ä»¥ä½¿ç”¨**ä¸åŒçš„è§†å›¾è¿›è¡Œè¯»å–å’Œå†™å…¥**ã€‚å› æ­¤ï¼Œå¤åˆ¶å’Œ GEMM æ“ä½œçš„çº¿ç¨‹å¸ƒå±€æ˜¯è§£è€¦çš„ï¼Œä»¥ä¾¿ç”¨æˆ·å¯ä»¥ä¸ºæ¯ä¸ªæ“ä½œé€‰æ‹©æœ€ä½³çš„é€‰é¡¹
CuTe è®¿é—®å…¨å±€å­˜å‚¨æ—¶å°½å¯èƒ½åˆå¹¶è®¿å­˜ï¼Œå°†æ•°æ®å­˜å‚¨åœ¨å…±äº«å­˜å‚¨æ—¶é¿å…å†æ¬¡è®¿é—®æ˜¯bank conflictã€‚


###  Incorporating TMA and WGMMA instructions from NVIDIA Hopper Architecture

The copy API call should be changed to include the TMA copy atom;
The gemm API call should be changed to include the MMA atom â€“ for Hopper, we choose WGM

```c++
....
for (int k = 0; k < size<1>(tAgA); ++k)
{
.....
//copy A and B from GMEM to SMEM using COPY views.
if (threadIdx.x == 0)
{
/// Initialize shared memory barrier
....
copy(tma_copy_a, tAgA(_,k), tAsA);
copy(tma_copy_b, tBgB(_,k), tBsB);
}
__syncthreads();
warpgroup_fence_operand(tCrC);
cute::gemm(wmma_atom, tCrA, tCrB, tCrC);
warpgroup_commit_batch();
warpgroup_wait<1>();
__syncthreads();
}
```


### ADDITIONAL OPTIMIZATIONS

ä»ç¬¬2èŠ‚å›é¡¾ï¼Œæœ€ä½³çš„CUTLASSæ ¸å¯¹äºSGEMMçš„æ€§èƒ½çº¦ä¸º280 TFLOPSï¼Œè€ŒcuBLASçš„æ€§èƒ½çº¦ä¸º215 TFLOPSã€‚CUTLASSé€šè¿‡å®ç°æ›´å¤šçš„ä¼˜åŒ–æ¥è¾¾åˆ°è¿™ä¸€ä¼˜è¶Šçš„æ€§èƒ½æ°´å¹³ã€‚ä»¥ä¸‹æ˜¯æ–‡æ¡£ä¸­æåˆ°çš„ä¸€äº›ä¼˜åŒ–æªæ–½[11]ï¼š

(1) è½¯ä»¶æµæ°´çº¿ â€“ è½¯ä»¶æµæ°´çº¿æ˜¯ä¸€ç§éšè—å†…å­˜å»¶è¿Ÿçš„æŠ€æœ¯ï¼Œå®ƒé€šè¿‡è®©å†…å­˜è®¿é—®å’Œæ•°å­¦æŒ‡ä»¤å¹¶å‘æ‰§è¡Œæ¥å®ç°ï¼ŒåŒæ—¶å§‹ç»ˆè€ƒè™‘è¿™äº›æ­¥éª¤ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚CUTLASSå®ç°ä½¿ç”¨äº†å¤šä¸ªç¼“å†²åŒºï¼Œæ—¢åŒ…æ‹¬çº¿ç¨‹å—å±‚é¢ï¼Œä¹ŸåŒ…æ‹¬warpå±‚é¢ã€‚

(2) Warpä¸“ä¸šåŒ– â€“ åœ¨è½¯ä»¶æµæ°´çº¿ç­‰ä¼˜åŒ–ä¸‹ï¼Œä¸åŒçš„çº¿ç¨‹æˆ–çº¿ç¨‹ç»„è‡ªç„¶å…·æœ‰ä¸åŒçš„è§’è‰²ã€‚ä¸€äº›çº¿ç¨‹æ˜¯ç”Ÿäº§è€…ï¼Œè´Ÿè´£åŠ è½½æ•°æ®ï¼Œè€Œå…¶ä»–çº¿ç¨‹æ˜¯æ¶ˆè´¹è€…ï¼Œè´Ÿè´£æ‰§è¡ŒMMAæŒ‡ä»¤ã€‚warpä¸“ä¸šåŒ–çš„æ€æƒ³æ˜¯å°†çº¿ç¨‹å—ä¸­çš„warpsç©ºé—´ä¸Šåˆ’åˆ†ä¸ºä¸¤ç»„ï¼Œåˆ†åˆ«ä½œä¸ºç”Ÿäº§è€…å’Œæ¶ˆè´¹è€…ã€‚

(3) æŒä¹…æ ¸ â€“ æŒä¹…æ ¸æ˜¯ä¸€ç§CUDAè®¾è®¡æ¨¡å¼ï¼Œæ—¨åœ¨é€šè¿‡è®©æ ¸åœ¨GPUä¸ŠæŒç»­å­˜åœ¨ä»¥é¿å…å†…æ ¸å¯åŠ¨å’Œé…ç½®çš„å¼€é”€ã€‚åœ¨CUTLASSä¸­ï¼Œè¿™æ„å‘³ç€æŒä¹…çº¿ç¨‹å—åœ¨å…¶ç”Ÿå‘½å‘¨æœŸå†…è®¡ç®—å¤šä¸ªè¾“å‡ºå—ã€‚

(4) ä¸¤ä¸ªåä½œæ¶ˆè´¹warpç»„ â€“ WGMMAå…è®¸æ“ä½œæ•°ğ´çš„å—å­˜æ”¾åœ¨å¯„å­˜å™¨å†…å­˜ä¸­ï¼Œè€Œä¸æ˜¯å…±äº«å†…å­˜ä¸­ã€‚ç„¶è€Œï¼Œè¿™é™åˆ¶äº†ğ´çš„å—å¤§å°ï¼Œå› ä¸ºå¯„å­˜å™¨ç©ºé—´æœ‰é™ã€‚å°†å—å¤§å°åœ¨ğ‘€ç»´åº¦ä¸Šæ‹†åˆ†å¹¶åˆ†é…ç»™ä¸¤ä¸ªä¸åŒçš„æ¶ˆè´¹warpç»„ï¼Œå¯ä»¥å…è®¸æ›´å¤§çš„å—å¤§å°å¹¶å‡è½»å¯„å­˜å™¨å‹åŠ›ã€‚

(5) Warpä¸“ä¸šåŒ–æŒä¹…ping-pongæ ¸ â€“ ä»(4)ä¸­çš„ä¸¤ä¸ªæ¶ˆè´¹warpç»„å„è‡ªåˆ†é…ç»™ä¸åŒçš„è¾“å‡ºå—ã€‚è¿™å…è®¸ä¸€ä¸ªæ¶ˆè´¹warpç»„çš„æœ€ç»ˆçº¢åˆ©ä¸å¦ä¸€ä¸ªæ¶ˆè´¹warpç»„çš„æ•°å­¦æ“ä½œé‡å ï¼Œä»è€Œæœ€å¤§åŒ–å¼ é‡æ ¸å¿ƒçš„åˆ©ç”¨ç‡ã€‚åŒæ—¶åœ¨ç”Ÿäº§è€…warpç»„ä¹‹é—´è¿›è¡ŒåŒæ­¥ã€‚

æ ¹æ®æˆ‘ä»¬çš„å®éªŒç ”ç©¶ï¼Œç‰¹åˆ«æ˜¯(5)è¿™ä¸€ç‚¹åœ¨ç¬¬2å›¾çš„ç¬¬å››åˆ—ä¸280 TFLOPSæœ€ä½³æµ‹é‡CUTLASSæ ¸ä¹‹é—´çš„å·®è·ä¸Šèµ·åˆ°äº†é‡è¦ä½œç”¨ã€‚


### cute æ‰©å±•æ€§
 BATCHED-GEMM
The AI workflow that we are targeting does not involve multiplying large square matrices. Instead, it involves
large square matrices decomposed as products of matrices with small ğ¾ (e.g., 64 or 128), and with batch count
ğ¿ > 1 (e.g., 64 or 96); cf. [1, Â§2.2]. Such a scheme is popularly known as Batched-GEMM. Our CuTe program can
be extended to handle Batched-GEMM by simply setting the third dimension of the grid to be ğ¿. We then use
blockIdx.z when using the local_tile operation inside the CUDA kernel, as shown in listing 4.
```
auto gA = local_tile(mA, make_shape(MT, KT), make_coord(blockIdx.x, _, blockIdx.
z));
auto gB = local_tile(mB, make_shape(NT, KT), make_coord(blockIdx.y, _, blockIdx.
z));
auto gC = local_tile(mC, make_shape(MT, NT), make_coord(blockIdx.x, blockIdx.y,
blockIdx.z);
```
Listing 4. Batched-GEMM kernel using CuTe
Performance of such a Batched-GEMM using CuTe is shown in Figure 6. Surprisingly, the CuTe program
outperforms both cuBLAS and CUTLASS, even though it does not use any of the additional optimizations that
CUTLASS uses as listed in Â§5.



Based on the layout of the shared memory and the mma instruction being used - we change the swizzle pattern to ensure both the stores and loads to and from shared memory are bank conflict free.
